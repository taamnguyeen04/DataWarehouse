{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings (Multi-GPU & Sharding Support)\n",
    "\n",
    "This notebook generates embeddings for the corpus using a trained Bi-Encoder model. \n",
    "It supports:\n",
    "- **Multi-GPU**: Automatically uses DataParallel if multiple GPUs are detected.\n",
    "- **Sharding**: Allows splitting the workload across multiple machines/notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = \"C:/Users/tam/Desktop/Data/Data Warehouse\"\n",
    "    CORPUS_FILE = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/corpus.jsonl\")\n",
    "    \n",
    "    # Model\n",
    "    MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    MAX_LENGTH = 512\n",
    "    EMBEDDING_DIM = 768\n",
    "    POOLING = \"mean\"  # 'cls', 'mean', or 'max'\n",
    "\n",
    "    # Output\n",
    "    OUTPUT_DIR = \"embeddings_output\"\n",
    "    \n",
    "    # Execution Parameters\n",
    "    MODEL_PATH = \"best_model.pt\"\n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    # Distributed Processing (Sharding)\n",
    "    # Set these values to split work across multiple machines\n",
    "    TOTAL_SHARDS = 1  # Total number of machines/notebooks\n",
    "    SHARD_ID = 0      # ID of this machine (0 to TOTAL_SHARDS-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition (BiEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "                 embedding_dim=768, pooling='cls'):\n",
    "        super(BiEncoder, self).__init__()\n",
    "\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.doc_encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def pool_embeddings(self, last_hidden_state, attention_mask):\n",
    "        if self.pooling == 'cls':\n",
    "            # Use [CLS] token embedding\n",
    "            return last_hidden_state[:, 0, :]\n",
    "\n",
    "        elif self.pooling == 'mean':\n",
    "            # Mean pooling with attention mask\n",
    "            token_embeddings = last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "\n",
    "        elif self.pooling == 'max':\n",
    "            # Max pooling\n",
    "            token_embeddings = last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "            return torch.max(token_embeddings, 1)[0]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {self.pooling}\")\n",
    "\n",
    "    def encode_query(self, input_ids, attention_mask):\n",
    "        # Encode query (patient summary)\n",
    "        outputs = self.query_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled = self.pool_embeddings(outputs.last_hidden_state, attention_mask)\n",
    "        return F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "    def encode_doc(self, input_ids, attention_mask):\n",
    "        # Encode document (article title + abstract)\n",
    "        outputs = self.doc_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        pooled = self.pool_embeddings(outputs.last_hidden_state, attention_mask)\n",
    "        return F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "    def forward(self, query_input_ids=None, query_attention_mask=None,\n",
    "                doc_input_ids=None, doc_attention_mask=None, mode='dual'):\n",
    "        if mode == 'dual':\n",
    "            query_embeddings = self.encode_query(query_input_ids, query_attention_mask)\n",
    "            doc_embeddings = self.encode_doc(doc_input_ids, doc_attention_mask)\n",
    "            return query_embeddings, doc_embeddings\n",
    "        elif mode == 'doc':\n",
    "            return self.encode_doc(doc_input_ids, doc_attention_mask)\n",
    "        elif mode == 'query':\n",
    "            return self.encode_query(query_input_ids, query_attention_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, corpus_file, tokenizer, max_length=384, start_idx=0, end_idx=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.lines = []\n",
    "        \n",
    "        print(f\"Loading corpus from line {start_idx} to {end_idx if end_idx else 'end'}...\")\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            # Skip to start_idx\n",
    "            for _ in range(start_idx):\n",
    "                next(f, None)\n",
    "            \n",
    "            # Read until end_idx\n",
    "            count = 0\n",
    "            limit = end_idx - start_idx if end_idx else float('inf')\n",
    "            \n",
    "            for line in f:\n",
    "                if count >= limit:\n",
    "                    break\n",
    "                self.lines.append(line)\n",
    "                count += 1\n",
    "        print(f\"Loaded {len(self.lines)} documents into memory.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = self.lines[idx]\n",
    "        try:\n",
    "            doc = json.loads(line)\n",
    "            doc_id = str(doc['_id'])\n",
    "            text = f\"{doc.get('title', '')} {doc.get('text', '')}\".strip()\n",
    "            \n",
    "            enc = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'doc_id': doc_id,\n",
    "                'input_ids': enc['input_ids'].squeeze(0),\n",
    "                'attention_mask': enc['attention_mask'].squeeze(0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'doc_id': [b['doc_id'] for b in batch],\n",
    "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Available GPUs: {num_gpus}\")\n",
    "\n",
    "    # 1. Calculate Shard Range\n",
    "    print(\"Counting total documents in corpus...\")\n",
    "    total_docs = 0\n",
    "    with open(Config.CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
    "        for _ in f:\n",
    "            total_docs += 1\n",
    "    print(f\"Total documents: {total_docs}\")\n",
    "    \n",
    "    docs_per_shard = math.ceil(total_docs / Config.TOTAL_SHARDS)\n",
    "    start_idx = Config.SHARD_ID * docs_per_shard\n",
    "    end_idx = min((Config.SHARD_ID + 1) * docs_per_shard, total_docs)\n",
    "    \n",
    "    print(f\"Shard {Config.SHARD_ID}/{Config.TOTAL_SHARDS}: Processing documents {start_idx} to {end_idx}\")\n",
    "    \n",
    "    if start_idx >= total_docs:\n",
    "        print(\"Shard ID out of range. Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # 2. Load Model\n",
    "    print(f\"Loading model from {Config.MODEL_PATH}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME)\n",
    "    model = BiEncoder(Config.MODEL_NAME, Config.EMBEDDING_DIM, Config.POOLING)\n",
    "    \n",
    "    if os.path.exists(Config.MODEL_PATH):\n",
    "        checkpoint = torch.load(Config.MODEL_PATH, map_location='cpu')\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        if list(state_dict.keys())[0].startswith('module.'):\n",
    "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Model loaded successfully.\")\n",
    "    else:\n",
    "        print(\"⚠️ Checkpoint not found! Using base model.\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable DataParallel if multiple GPUs are available\n",
    "    if num_gpus > 1:\n",
    "        print(f\"Enabling DataParallel on {num_gpus} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    # 3. Prepare Data\n",
    "    dataset = CorpusDataset(Config.CORPUS_FILE, tokenizer, start_idx=start_idx, end_idx=end_idx)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=Config.BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    output_file = os.path.join(Config.OUTPUT_DIR, f\"corpus_embeddings_shard_{Config.SHARD_ID}.jsonl\")\n",
    "\n",
    "    print(f\"Generating embeddings to {output_file}...\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader):\n",
    "                if batch is None: continue\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                doc_ids = batch['doc_id']\n",
    "\n",
    "                # Encode\n",
    "                if num_gpus > 1:\n",
    "                    # DataParallel wraps the model, so we call forward directly or access module\n",
    "                    # BiEncoder.forward handles 'mode' argument\n",
    "                    embeddings = model(doc_input_ids=input_ids, doc_attention_mask=attention_mask, mode='doc')\n",
    "                else:\n",
    "                    embeddings = model.encode_doc(input_ids, attention_mask)\n",
    "                \n",
    "                embeddings = embeddings.cpu().numpy()\n",
    "\n",
    "                # Write to file\n",
    "                for doc_id, emb in zip(doc_ids, embeddings):\n",
    "                    record = {\n",
    "                        \"pmid\": doc_id,\n",
    "                        \"embedding\": emb.tolist()\n",
    "                    }\n",
    "                    f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Done! Shard {Config.SHARD_ID} saved to {output_file}\")\n",
    "\n",
    "# Run\n",
    "generate_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
