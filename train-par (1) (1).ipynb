{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a65b5b",
   "metadata": {},
   "source": [
    "# Welcome to Modal notebooks!\n",
    "\n",
    "Write Python code and collaborate in real time. Your code runs in Modal's\n",
    "**serverless cloud**, and anyone in the same workspace can join.\n",
    "\n",
    "This notebook comes with some common Python libraries installed. Run\n",
    "cells with `Shift+Enter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "946b9f57-3b78-4f45-9116-2804466afc9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (2.2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.9.1+cu126)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.24.1+cu126)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.local/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.local/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.local/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.local/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.local/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.local/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.local/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.local/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.1 in ./.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in ./.local/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.local/lib/python3.10/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.local/lib/python3.10/site-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.10/site-packages (from tensorboard) (3.10)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.local/lib/python3.10/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.10/site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.local/lib/python3.10/site-packages (from tensorboard) (6.33.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (75.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.local/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.local/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.10/dist-packages (from grpcio>=1.48.2->tensorboard) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install tqdm\n",
    "!pip install transformers\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5d9cc0-043b-40d1-bc9d-9fa5531de0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n",
      "/usr/bin/python3\n",
      "/usr/local/bin/pip\n",
      "/usr/local/bin/pip3\n",
      "/usr/bin/python3\n",
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/admin/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages']\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which python3\n",
    "!which pip\n",
    "!which pip3\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a486fedd-71a3-49e9-87ba-43924da45b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/admin/.local/lib/python3.10/site-packages\")\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5603533f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_172511/2714838516.py:15: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1111588b-bd34-4c34-95cf-53ae6dd58615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff38935b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_DIR = \"/workspace/Data Warehouse\"\n",
    "    CORPUS_FILE = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/corpus.jsonl\")\n",
    "    TRAIN_QUERIES = os.path.join(DATA_DIR, \"ReCDS_benchmark/queries/train_queries.jsonl\")\n",
    "    TRAIN_QRELS = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/qrels_train.tsv\")\n",
    "    DEV_QUERIES = os.path.join(DATA_DIR, \"ReCDS_benchmark/queries/dev_queries.jsonl\")\n",
    "    DEV_QRELS = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/qrels_dev.tsv\")\n",
    "    BM25_HARD_NEGS_FILE = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/gold/bm25_hard_negs.json\")\n",
    "    PAIRS_TRAIN_FILE = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/gold/pairs_train.jsonl\")\n",
    "\n",
    "    # Model\n",
    "    MODEL_NAME = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    MAX_LENGTH = 384\n",
    "    EMBEDDING_DIM = 768\n",
    "    POOLING = \"mean\"\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 324\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.1\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    TEMPERATURE = 0.05\n",
    "    NUM_HARD_NEGATIVES = 2\n",
    "    USE_MIXED_PRECISION = True\n",
    "    # System\n",
    "    NUM_WORKERS = 120\n",
    "    CHECKPOINT_DIR = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/checkpoints\")\n",
    "    LOG_DIR = os.path.join(DATA_DIR, \"ReCDS_benchmark/PAR/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3046ba",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PARDatasetOptimized(Dataset):\n",
    "    def __init__(self, queries_file, qrels_file, corpus_file, tokenizer, max_length=512,\n",
    "                 bm25_hard_negs_file=None, pairs_train_file=None, num_hard_negatives=2):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_hard_negatives = num_hard_negatives\n",
    "\n",
    "        print(\"Loading queries...\")\n",
    "        self.queries = {}\n",
    "        with open(queries_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                query = json.loads(line.strip())\n",
    "                self.queries[query['_id']] = query['text']\n",
    "        print(f\"Loaded {len(self.queries)} queries\")\n",
    "\n",
    "        # Load BM25 hard negatives\n",
    "        # self.bm25_hard_negs = {}\n",
    "        # if bm25_hard_negs_file:\n",
    "        #     print(f\"Loading BM25 hard negatives...\")\n",
    "        #     with open(bm25_hard_negs_file, 'r', encoding='utf-8') as f:\n",
    "        #         self.bm25_hard_negs = json.load(f)\n",
    "        #     print(f\"Loaded BM25 hard negatives for {len(self.bm25_hard_negs)} queries\")\n",
    "\n",
    "        # Load training pairs\n",
    "        # if pairs_train_file:\n",
    "        print(f\"Loading training pairs...\")\n",
    "        self.pairs = []\n",
    "        with open(pairs_train_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                pair = json.loads(line.strip())\n",
    "                self.pairs.append({\n",
    "                    'query_id': pair['query_id'],\n",
    "                    'pos_id': pair['pos_id'],\n",
    "                    'neg_ids': pair.get('neg_ids', [])\n",
    "                })\n",
    "        print(f\"Loaded {len(self.pairs)} training pairs\")\n",
    "        # else:\n",
    "        #     print(\"Loading qrels...\")\n",
    "        #     qrels = {}\n",
    "        #     with open(qrels_file, 'r', encoding='utf-8') as f:\n",
    "        #         for line in f:\n",
    "        #             parts = line.strip().split('\\t')\n",
    "        #             if len(parts) == 3:\n",
    "        #                 query_id, doc_id, relevance = parts\n",
    "        #                 try:\n",
    "        #                     if int(relevance) > 0:\n",
    "        #                         if query_id not in qrels:\n",
    "        #                             qrels[query_id] = []\n",
    "        #                         qrels[query_id].append(doc_id)\n",
    "        #                 except ValueError:\n",
    "        #                     continue\n",
    "\n",
    "            # print(\"Creating training pairs...\")\n",
    "            # self.pairs = []\n",
    "            # for query_id, doc_ids in qrels.items():\n",
    "            #     if query_id in self.queries:\n",
    "            #         for doc_id in doc_ids:\n",
    "            #             neg_ids = self.bm25_hard_negs.get(query_id, [])[:self.num_hard_negatives]\n",
    "            #             self.pairs.append({\n",
    "            #                 'query_id': query_id,\n",
    "            #                 'pos_id': doc_id,\n",
    "            #                 'neg_ids': neg_ids\n",
    "            #             })\n",
    "            # print(f\"Created {len(self.pairs)} pairs\")\n",
    "\n",
    "        # Load required documents\n",
    "        needed_doc_ids = set()\n",
    "        for pair in self.pairs:\n",
    "            needed_doc_ids.add(pair['pos_id'])\n",
    "            needed_doc_ids.update(pair['neg_ids'])\n",
    "        print(f\"Need to load {len(needed_doc_ids)} documents\")\n",
    "\n",
    "        print(f\"Loading documents...\")\n",
    "        self.corpus = {}\n",
    "        loaded_count = 0\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                doc = json.loads(line.strip())\n",
    "                doc_id = str(doc.get('_id', ''))\n",
    "                if doc_id in needed_doc_ids:\n",
    "                    title = doc.get('title', '').strip()\n",
    "                    abstract = doc.get('text', '').strip()\n",
    "                    if title or abstract:\n",
    "                        self.corpus[doc_id] = {'title': title, 'abstract': abstract}\n",
    "                        loaded_count += 1\n",
    "                # if loaded_count >= len(needed_doc_ids):\n",
    "                #     break\n",
    "\n",
    "        print(f\"Loaded {len(self.corpus)} documents\")\n",
    "\n",
    "        # Filter pairs\n",
    "        self.pairs = [\n",
    "            {\n",
    "                'query_id': p['query_id'],\n",
    "                'pos_id': p['pos_id'],\n",
    "                'neg_ids': [n for n in p['neg_ids'] if n in self.corpus]\n",
    "            }\n",
    "            for p in self.pairs\n",
    "            if p['query_id'] in self.queries and p['pos_id'] in self.corpus\n",
    "        ]\n",
    "        print(f\"Filtered to {len(self.pairs)} valid pairs\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        query_id = pair['query_id']\n",
    "        pos_id = pair['pos_id']\n",
    "        neg_ids = pair['neg_ids']\n",
    "    \n",
    "        # âœ… Kiá»ƒm tra dá»¯ liá»‡u bá»‹ thiáº¿u\n",
    "        if query_id not in self.queries or pos_id not in self.corpus:\n",
    "            return None\n",
    "    \n",
    "        query_text = self.queries[query_id]\n",
    "        pos_doc = self.corpus[pos_id]\n",
    "        pos_text = f\"{pos_doc['title']} {pos_doc['abstract']}\".strip()\n",
    "    \n",
    "        query_enc = self.tokenizer(\n",
    "            query_text, max_length=self.max_length, padding='max_length',\n",
    "            truncation=True, return_tensors='pt'\n",
    "        )\n",
    "        pos_enc = self.tokenizer(\n",
    "            pos_text, max_length=self.max_length, padding='max_length',\n",
    "            truncation=True, return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "        result = {\n",
    "            'query_input_ids': query_enc['input_ids'].squeeze(0),\n",
    "            'query_attention_mask': query_enc['attention_mask'].squeeze(0),\n",
    "            'pos_doc_input_ids': pos_enc['input_ids'].squeeze(0),\n",
    "            'pos_doc_attention_mask': pos_enc['attention_mask'].squeeze(0),\n",
    "        }\n",
    "    \n",
    "        # Encode hard negatives náº¿u cÃ³\n",
    "        if neg_ids:\n",
    "            neg_ids_list = []\n",
    "            neg_masks_list = []\n",
    "            for neg_id in neg_ids[:self.num_hard_negatives]:\n",
    "                if neg_id in self.corpus:\n",
    "                    neg_doc = self.corpus[neg_id]\n",
    "                    neg_text = f\"{neg_doc['title']} {neg_doc['abstract']}\".strip()\n",
    "                    neg_enc = self.tokenizer(\n",
    "                        neg_text, max_length=self.max_length,\n",
    "                        padding='max_length', truncation=True, return_tensors='pt'\n",
    "                    )\n",
    "                    neg_ids_list.append(neg_enc['input_ids'].squeeze(0))\n",
    "                    neg_masks_list.append(neg_enc['attention_mask'].squeeze(0))\n",
    "    \n",
    "            if neg_ids_list:\n",
    "                result['neg_doc_input_ids'] = torch.stack(neg_ids_list)\n",
    "                result['neg_doc_attention_mask'] = torch.stack(neg_masks_list)\n",
    "            else:\n",
    "                result['neg_doc_input_ids'] = torch.empty(0, self.max_length, dtype=torch.long)\n",
    "                result['neg_doc_attention_mask'] = torch.empty(0, self.max_length, dtype=torch.long)\n",
    "        else:\n",
    "            result['neg_doc_input_ids'] = torch.empty(0, self.max_length, dtype=torch.long)\n",
    "            result['neg_doc_attention_mask'] = torch.empty(0, self.max_length, dtype=torch.long)\n",
    "    \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55930c59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BiEncoder(nn.Module):\n",
    "    def __init__(self, model_name, embedding_dim, pooling='mean'):\n",
    "        super().__init__()\n",
    "        self.query_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.doc_encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def pool_embeddings(self, last_hidden_state, attention_mask):\n",
    "        if self.pooling == 'cls':\n",
    "            return last_hidden_state[:, 0, :]\n",
    "        elif self.pooling == 'mean':\n",
    "            token_embeddings = last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            return sum_embeddings / sum_mask\n",
    "        elif self.pooling == 'max':\n",
    "            token_embeddings = last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "            return torch.max(token_embeddings, 1)[0]\n",
    "\n",
    "    def encode_query(self, input_ids, attention_mask):\n",
    "        outputs = self.query_encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = self.pool_embeddings(outputs.last_hidden_state, attention_mask)\n",
    "        return F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "    def encode_doc(self, input_ids, attention_mask):\n",
    "        outputs = self.doc_encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = self.pool_embeddings(outputs.last_hidden_state, attention_mask)\n",
    "        return F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "    def forward(self, query_input_ids=None, query_attention_mask=None,\n",
    "                doc_input_ids=None, doc_attention_mask=None, mode='dual'):\n",
    "        if mode == 'dual':\n",
    "            return self.encode_query(query_input_ids, query_attention_mask), \\\n",
    "                   self.encode_doc(doc_input_ids, doc_attention_mask)\n",
    "        elif mode == 'doc':\n",
    "            return self.encode_doc(doc_input_ids, doc_attention_mask)\n",
    "        elif mode == 'query':\n",
    "            return self.encode_query(query_input_ids, query_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc456f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, query_embeddings, doc_embeddings, hard_neg_embeddings=None):\n",
    "        batch_size = query_embeddings.size(0)\n",
    "        similarity_matrix = torch.matmul(query_embeddings, doc_embeddings.T) / self.temperature\n",
    "\n",
    "        if hard_neg_embeddings is not None and hard_neg_embeddings.size(1) > 0:\n",
    "            hard_neg_sim = torch.bmm(\n",
    "                query_embeddings.unsqueeze(1),\n",
    "                hard_neg_embeddings.transpose(1, 2)\n",
    "            ).squeeze(1) / self.temperature\n",
    "            similarity_matrix = torch.cat([similarity_matrix, hard_neg_sim], dim=1)\n",
    "\n",
    "        labels = torch.arange(batch_size, device=query_embeddings.device)\n",
    "        loss_q2d = self.criterion(similarity_matrix, labels)\n",
    "        \n",
    "        doc2query_sim = torch.matmul(doc_embeddings, query_embeddings.T) / self.temperature\n",
    "        loss_d2q = self.criterion(doc2query_sim, labels)\n",
    "        \n",
    "        return (loss_q2d + loss_d2q) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07757093",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "Using device: cuda\n",
      "Available GPUs: 3\n",
      "âœ“ Mixed Precision Training ENABLED (FP16) - Expect ~2x speedup\n",
      "Using DataParallel on 3 GPUs\n",
      "Loading queries...\n",
      "Loaded 155151 queries\n",
      "Loading training pairs...\n",
      "Loaded 1978118 training pairs\n",
      "Need to load 993196 documents\n",
      "Loading documents...\n",
      "Loaded 993196 documents\n",
      "Filtered to 1978118 valid pairs\n",
      "Loading queries...\n",
      "Loaded 5924 queries\n",
      "Loading training pairs...\n",
      "Loaded 1978118 training pairs\n",
      "Need to load 993196 documents\n",
      "Loading documents...\n",
      "Loaded 993196 documents\n",
      "Filtered to 0 valid pairs\n",
      "Loaded from last_model.pt epoch 0\n",
      "load.....\n",
      "1 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4768f5dd268c45ccae13b927c3c63dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved checkpoint at step 7200 â†’ /workspace/Data Warehouse/ReCDS_benchmark/PAR/checkpoints/step_7200.pt\n",
      "ðŸ’¾ Saved checkpoint at step 8400 â†’ /workspace/Data Warehouse/ReCDS_benchmark/PAR/checkpoints/step_8400.pt\n",
      "ðŸ’¾ Saved checkpoint at step 9600 â†’ /workspace/Data Warehouse/ReCDS_benchmark/PAR/checkpoints/step_9600.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 366\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 366\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 252\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_amp:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m--> 252\u001b[0m         query_embeddings, doc_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_input_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery_attention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos_doc_input_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpos_doc_attention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m         hard_neg_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_doc_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch \u001b[38;5;129;01mand\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_doc_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 193\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreplicate\u001b[39m(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m, module: T, device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]]\n\u001b[1;32m    199\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[T]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreplicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/replicate.py:144\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m    139\u001b[0m buffer_copies_rg \u001b[38;5;241m=\u001b[39m _broadcast_coalesced_reshape(buffers_rg, devices, detach\u001b[38;5;241m=\u001b[39mdetach)\n\u001b[1;32m    140\u001b[0m buffer_copies_not_rg \u001b[38;5;241m=\u001b[39m _broadcast_coalesced_reshape(\n\u001b[1;32m    141\u001b[0m     buffers_not_rg, devices, detach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m module_copies: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Module]] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[1;32m    146\u001b[0m module_indices: \u001b[38;5;28mdict\u001b[39m[Module, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2820\u001b[0m, in \u001b[0;36mModule.modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodules\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over all modules in the network.\u001b[39;00m\n\u001b[1;32m   2798\u001b[0m \n\u001b[1;32m   2799\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2818\u001b[0m \n\u001b[1;32m   2819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2820\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m   2821\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m module\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2868\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2867\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2868\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2869\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2870\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2868\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2867\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2868\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2869\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2870\u001b[0m )\n",
      "    \u001b[0;31m[... skipping similar frames: Module.named_modules at line 2868 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2868\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2867\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 2868\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(\n\u001b[1;32m   2869\u001b[0m     memo, submodule_prefix, remove_duplicate\n\u001b[1;32m   2870\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2863\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n\u001b[1;32m   2862\u001b[0m     memo\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 2863\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_metrics(query_embeddings, doc_embeddings, query_ids, doc_ids, qrels_dict, k_values=[10, 100, 1000]):\n",
    "    similarity_matrix = torch.matmul(query_embeddings, doc_embeddings.T).cpu().numpy()\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    for i, query_id in enumerate(query_ids):\n",
    "        relevant_docs = qrels_dict.get(query_id, set())\n",
    "        if not relevant_docs:\n",
    "            continue\n",
    "\n",
    "        scores = similarity_matrix[i]\n",
    "        sorted_indices = np.argsort(-scores)\n",
    "        sorted_doc_ids = [doc_ids[idx] for idx in sorted_indices]\n",
    "\n",
    "        for k in k_values:\n",
    "            top_k_docs = sorted_doc_ids[:k]\n",
    "            num_relevant = len(set(top_k_docs) & relevant_docs)\n",
    "            \n",
    "            metrics[f'Recall@{k}'].append(num_relevant / len(relevant_docs))\n",
    "            metrics[f'P@{k}'].append(num_relevant / k)\n",
    "            \n",
    "            dcg = sum(1.0 / np.log2(rank + 2) for rank, doc_id in enumerate(top_k_docs) if doc_id in relevant_docs)\n",
    "            idcg = sum(1.0 / np.log2(rank + 2) for rank in range(min(len(relevant_docs), k)))\n",
    "            metrics[f'nDCG@{k}'].append(dcg / idcg if idcg > 0 else 0.0)\n",
    "\n",
    "        for rank, doc_id in enumerate(sorted_doc_ids, start=1):\n",
    "            if doc_id in relevant_docs:\n",
    "                metrics['MRR'].append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            metrics['MRR'].append(0.0)\n",
    "\n",
    "    return {name: np.mean(values) for name, values in metrics.items()}\n",
    "\n",
    "\n",
    "def evaluate_full_corpus(model, dataset, device, batch_size=32, use_amp=False):\n",
    "    \"\"\"\n",
    "    ThÃªm tham sá»‘ use_amp Ä‘á»ƒ há»— trá»£ mixed precision trong quÃ¡ trÃ¬nh evaluation\n",
    "    \"\"\"\n",
    "    from torch.cuda.amp import autocast\n",
    "    \n",
    "    model.eval()\n",
    "    tokenizer = dataset.tokenizer\n",
    "    max_length = dataset.max_length\n",
    "\n",
    "    # Build qrels\n",
    "    qrels_dict = defaultdict(set)\n",
    "    for pair in dataset.pairs:\n",
    "        qrels_dict[pair['query_id']].add(pair['pos_id'])\n",
    "\n",
    "    # Encode queries\n",
    "    query_ids = list(dataset.queries.keys())\n",
    "    query_embeddings_list = []\n",
    "    print(\"Encoding queries...\")\n",
    "    for i in tqdm(range(0, len(query_ids), batch_size)):\n",
    "        batch_ids = query_ids[i:i + batch_size]\n",
    "        batch_texts = [dataset.queries[qid] for qid in batch_ids]\n",
    "        enc = tokenizer(batch_texts, max_length=max_length, padding='max_length', \n",
    "                       truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    embs = model(query_input_ids=enc['input_ids'].to(device),\n",
    "                                query_attention_mask=enc['attention_mask'].to(device), mode='query')\n",
    "            else:\n",
    "                embs = model(query_input_ids=enc['input_ids'].to(device),\n",
    "                            query_attention_mask=enc['attention_mask'].to(device), mode='query')\n",
    "            query_embeddings_list.append(embs.cpu())\n",
    "    query_embeddings = torch.cat(query_embeddings_list, dim=0)\n",
    "\n",
    "    # Encode documents\n",
    "    doc_ids = list(dataset.corpus.keys())\n",
    "    doc_embeddings_list = []\n",
    "    print(\"Encoding documents...\")\n",
    "    for i in tqdm(range(0, len(doc_ids), batch_size)):\n",
    "        batch_ids = doc_ids[i:i + batch_size]\n",
    "        batch_texts = [f\"{dataset.corpus[did]['title']} {dataset.corpus[did]['abstract']}\".strip() \n",
    "                      for did in batch_ids]\n",
    "        enc = tokenizer(batch_texts, max_length=max_length, padding='max_length',\n",
    "                       truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    embs = model(doc_input_ids=enc['input_ids'].to(device),\n",
    "                                doc_attention_mask=enc['attention_mask'].to(device), mode='doc')\n",
    "            else:\n",
    "                embs = model(doc_input_ids=enc['input_ids'].to(device),\n",
    "                            doc_attention_mask=enc['attention_mask'].to(device), mode='doc')\n",
    "            doc_embeddings_list.append(embs.cpu())\n",
    "    doc_embeddings = torch.cat(doc_embeddings_list, dim=0)\n",
    "\n",
    "    return compute_metrics(query_embeddings, doc_embeddings, query_ids, doc_ids, qrels_dict)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(filepath, epoch, step, model, optimizer, loss, ndcg_at_10=None, scaler=None):\n",
    "    model_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'step': step,\n",
    "        'loss': loss,\n",
    "        'ndcg_at_10': ndcg_at_10 if ndcg_at_10 is not None else 0.0,\n",
    "        'model_state_dict': model_state,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Save scaler state if using mixed precision\n",
    "    if scaler is not None:\n",
    "        checkpoint['scaler_state_dict'] = scaler.state_dict()\n",
    "\n",
    "    last_path = os.path.join(filepath, \"last_model.pt\")\n",
    "    torch.save(checkpoint, last_path)\n",
    "\n",
    "    best_path = os.path.join(filepath, \"best_model.pt\")\n",
    "    if not os.path.exists(best_path):\n",
    "        torch.save(checkpoint, best_path)\n",
    "    else:\n",
    "        best_checkpoint = torch.load(best_path, map_location='cpu')\n",
    "        best_ndcg = best_checkpoint.get('ndcg_at_10', 0.0)\n",
    "        if ndcg_at_10 is not None and ndcg_at_10 > best_ndcg:\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"Updated best model: nDCG@10 {best_ndcg:.4f} â†’ {ndcg_at_10:.4f}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer, device, scaler=None):\n",
    "    last_path = \"last_model.pt\"\n",
    "    best_path = \"best_model.pt\"\n",
    "    import numpy as np\n",
    "    torch.serialization.add_safe_globals([np._core.multiarray.scalar])\n",
    "    model_to_load = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    \n",
    "    if os.path.isfile(last_path):\n",
    "        checkpoint = torch.load(last_path, map_location=device, weights_only=False)\n",
    "        model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        print(f\"Loaded from last_model.pt epoch {checkpoint['epoch']}\")\n",
    "        return checkpoint['epoch'] + 1, checkpoint.get('step', 0), \\\n",
    "               checkpoint.get('loss', float('inf')), checkpoint.get('ndcg_at_10', 0.0)\n",
    "    \n",
    "    if os.path.isfile(best_path):\n",
    "        checkpoint = torch.load(best_path, map_location=device, weights_only=False)\n",
    "        model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scaler is not None and 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        print(f\"Loaded from best_model.pt epoch {checkpoint['epoch']}\")\n",
    "        return checkpoint['epoch'] + 1, checkpoint.get('step', 0), \\\n",
    "               checkpoint.get('loss', float('inf')), checkpoint.get('ndcg_at_10', 0.0)\n",
    "    \n",
    "    print(\"No checkpoint found, starting from scratch\")\n",
    "    return 0, 0, float('inf'), 0.0\n",
    "\n",
    "def collate_skip_none(batch):\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.default_collate(batch)\n",
    "    \n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f\"Available GPUs: {n_gpus}\")\n",
    "    \n",
    "    use_amp = Config.USE_MIXED_PRECISION and device.type == 'cuda'\n",
    "    if use_amp:\n",
    "        print(\"âœ“ Mixed Precision Training ENABLED (FP16) - Expect ~2x speedup\")\n",
    "    else:\n",
    "        print(\"âœ— Mixed Precision Training DISABLED\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, use_fast=True)\n",
    "    model = BiEncoder(Config.MODEL_NAME, Config.EMBEDDING_DIM, Config.POOLING).to(device)\n",
    "    \n",
    "    if n_gpus > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        print(f\"Using DataParallel on {n_gpus} GPUs\")\n",
    "\n",
    "    train_dataset = PARDatasetOptimized(\n",
    "        queries_file=Config.TRAIN_QUERIES,\n",
    "        qrels_file=Config.TRAIN_QRELS,\n",
    "        corpus_file=Config.CORPUS_FILE,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        bm25_hard_negs_file=Config.BM25_HARD_NEGS_FILE,\n",
    "        pairs_train_file=Config.PAIRS_TRAIN_FILE,\n",
    "        num_hard_negatives=Config.NUM_HARD_NEGATIVES\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=collate_skip_none,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    dev_dataset = PARDatasetOptimized(\n",
    "        queries_file=Config.DEV_QUERIES,\n",
    "        qrels_file=Config.DEV_QRELS,\n",
    "        corpus_file=Config.CORPUS_FILE,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=Config.MAX_LENGTH,\n",
    "        bm25_hard_negs_file=None,  # Dev khÃ´ng cáº§n hard negatives\n",
    "        pairs_train_file=Config.PAIRS_TRAIN_FILE,\n",
    "        num_hard_negatives=0\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        num_workers=Config.NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "\n",
    "    criterion = InfoNCELoss(Config.TEMPERATURE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=Config.LEARNING_RATE, \n",
    "                                 weight_decay=Config.WEIGHT_DECAY)\n",
    "\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    total_steps = len(train_dataloader) * Config.NUM_EPOCHS\n",
    "    warmup_steps = int(total_steps * Config.WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    if os.path.isdir(Config.LOG_DIR):\n",
    "        shutil.rmtree(Config.LOG_DIR)\n",
    "    os.makedirs(Config.LOG_DIR)\n",
    "    os.makedirs(Config.CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(Config.LOG_DIR)\n",
    "    start_epoch, global_step, best_loss, best_ndcg = load_checkpoint(\n",
    "        Config.CHECKPOINT_DIR, model, optimizer, device, scaler\n",
    "    )\n",
    "    print(\"load.....\")\n",
    "    print(start_epoch, Config.NUM_EPOCHS)\n",
    "    for epoch in range(start_epoch, 10):\n",
    "        model.train()\n",
    "        all_losses = []\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, colour=\"BLUE\")\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    query_embeddings, doc_embeddings = model(\n",
    "                        batch['query_input_ids'].to(device),\n",
    "                        batch['query_attention_mask'].to(device),\n",
    "                        batch['pos_doc_input_ids'].to(device),\n",
    "                        batch['pos_doc_attention_mask'].to(device)\n",
    "                    )\n",
    "\n",
    "                    hard_neg_embeddings = None\n",
    "                    if 'neg_doc_input_ids' in batch and batch['neg_doc_input_ids'].size(1) > 0:\n",
    "                        neg_ids = batch['neg_doc_input_ids'].to(device)\n",
    "                        neg_mask = batch['neg_doc_attention_mask'].to(device)\n",
    "                        batch_size, num_negs, max_len = neg_ids.size()\n",
    "                        \n",
    "                        neg_embs = model(doc_input_ids=neg_ids.view(-1, max_len),\n",
    "                                        doc_attention_mask=neg_mask.view(-1, max_len), mode='doc')\n",
    "                        hard_neg_embeddings = neg_embs.view(batch_size, num_negs, -1)\n",
    "\n",
    "                    loss = criterion(query_embeddings, doc_embeddings, hard_neg_embeddings)\n",
    "                \n",
    "                # Backward with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard FP32 training\n",
    "                query_embeddings, doc_embeddings = model(\n",
    "                    batch['query_input_ids'].to(device),\n",
    "                    batch['query_attention_mask'].to(device),\n",
    "                    batch['pos_doc_input_ids'].to(device),\n",
    "                    batch['pos_doc_attention_mask'].to(device)\n",
    "                )\n",
    "\n",
    "                hard_neg_embeddings = None\n",
    "                if 'neg_doc_input_ids' in batch and batch['neg_doc_input_ids'].size(1) > 0:\n",
    "                    neg_ids = batch['neg_doc_input_ids'].to(device)\n",
    "                    neg_mask = batch['neg_doc_attention_mask'].to(device)\n",
    "                    batch_size, num_negs, max_len = neg_ids.size()\n",
    "                    \n",
    "                    neg_embs = model(doc_input_ids=neg_ids.view(-1, max_len),\n",
    "                                    doc_attention_mask=neg_mask.view(-1, max_len), mode='doc')\n",
    "                    hard_neg_embeddings = neg_embs.view(batch_size, num_negs, -1)\n",
    "\n",
    "                loss = criterion(query_embeddings, doc_embeddings, hard_neg_embeddings)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch+1}/{Config.NUM_EPOCHS} Loss {loss.item():.4f} LR {scheduler.get_last_lr()[0]:.2e}\"\n",
    "            )\n",
    "\n",
    "            all_losses.append(loss.item())\n",
    "            writer.add_scalar(\"Train/loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Train/learning_rate\", scheduler.get_last_lr()[0], global_step)\n",
    "            global_step += 1\n",
    "            if global_step % 1200 == 0:\n",
    "                ckpt_path = os.path.join(Config.CHECKPOINT_DIR, f\"step_{global_step}.pt\")\n",
    "                save_checkpoint(Config.CHECKPOINT_DIR, epoch, global_step, model, optimizer, \n",
    "                                loss.item(), None, scaler)\n",
    "                print(f\"ðŸ’¾ Saved checkpoint at step {global_step} â†’ {ckpt_path}\")\n",
    "        train_loss = np.mean(all_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        dev_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dev_dataloader, desc=\"Dev loss\"):\n",
    "                if use_amp:\n",
    "                    with autocast():\n",
    "                        query_embs, doc_embs = model(\n",
    "                            batch['query_input_ids'].to(device),\n",
    "                            batch['query_attention_mask'].to(device),\n",
    "                            batch['pos_doc_input_ids'].to(device),\n",
    "                            batch['pos_doc_attention_mask'].to(device)\n",
    "                        )\n",
    "                        loss = criterion(query_embs, doc_embs, None)\n",
    "                else:\n",
    "                    query_embs, doc_embs = model(\n",
    "                        batch['query_input_ids'].to(device),\n",
    "                        batch['query_attention_mask'].to(device),\n",
    "                        batch['pos_doc_input_ids'].to(device),\n",
    "                        batch['pos_doc_attention_mask'].to(device)\n",
    "                    )\n",
    "                    loss = criterion(query_embs, doc_embs, None)\n",
    "                dev_losses.append(loss.item())\n",
    "\n",
    "        dev_loss = np.mean(dev_losses)\n",
    "        print(f\"Epoch {epoch+1} - Dev Loss: {dev_loss:.4f}\")\n",
    "\n",
    "        # dev_metrics = evaluate_full_corpus(model, dev_dataset, device, use_amp=use_amp)\n",
    "        # print(f\"\\nDev Metrics:\")\n",
    "        # print(f\"  MRR:       {dev_metrics.get('MRR', 0):.4f}\")\n",
    "        # print(f\"  nDCG@10:   {dev_metrics.get('nDCG@10', 0):.4f}\")\n",
    "        # print(f\"  Recall@1k: {dev_metrics.get('Recall@1000', 0):.4f}\")\n",
    "\n",
    "        # writer.add_scalar(\"Dev/loss\", dev_loss, epoch)\n",
    "        # writer.add_scalar(\"Dev/MRR\", dev_metrics.get('MRR', 0), epoch)\n",
    "        # writer.add_scalar(\"Dev/nDCG@10\", dev_metrics.get('nDCG@10', 0), epoch)\n",
    "        # writer.add_scalar(\"Dev/Recall@1k\", dev_metrics.get('Recall@1000', 0), epoch)\n",
    "\n",
    "        save_checkpoint(Config.CHECKPOINT_DIR, epoch, global_step, model, optimizer, \n",
    "                       dev_loss, dev_metrics.get('nDCG@10', 0), scaler)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"run\")\n",
    "    train()\n",
    "    print(\"xong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe28edf-1811-42fb-b99c-8116a0fb96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0c611",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810d1b3-8536-42c4-9c52-6439405af4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
