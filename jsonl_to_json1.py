import os
import json
import glob
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# C·∫•u h√¨nh
INPUT_DIR = r"C:\Users\tam\Documents\data\Data Warehouse\embed\results (12)\embeddings_output"
OUTPUT_MERGED_DIR = r"C:\Users\tam\Documents\data\Data Warehouse\embed\results (12)\merged_output"
MAX_WORKERS = 16  # S·ªë lu·ªìng x·ª≠ l√Ω song song
FILES_PER_MERGE = 4  # S·ªë file JSON gh√©p th√†nh 1

# Lock ƒë·ªÉ in log an to√†n
print_lock = Lock()


def safe_print(message):
    """In log thread-safe"""
    with print_lock:
        print(message)


def process_jsonl_file(jsonl_path):
    """
    X·ª≠ l√Ω m·ªôt file JSONL: ƒë·ªçc, chuy·ªÉn ƒë·ªïi sang JSON, l∆∞u v√† x√≥a file g·ªëc

    Args:
        jsonl_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file JSONL

    Returns:
        tuple: (success, file_name, json_path, message)
    """
    try:
        jsonl_path = Path(jsonl_path)
        file_name = jsonl_path.name

        safe_print(f"[START] ƒêang x·ª≠ l√Ω: {file_name}")

        # ƒê·ªçc file JSONL
        data = []
        line_count = 0

        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        data.append(json.loads(line))
                        line_count += 1
                    except json.JSONDecodeError as e:
                        safe_print(f"[WARNING] L·ªói parse JSON t·∫°i d√≤ng {line_count + 1} trong {file_name}: {e}")

        if not data:
            safe_print(f"[WARNING] File {file_name} kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá")
            return (False, file_name, None, "No valid data")

        # T·∫°o t√™n file JSON output
        json_path = jsonl_path.with_suffix('.json')

        # Ghi file JSON
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # Ki·ªÉm tra k√≠ch th∆∞·ªõc file output
        file_size_mb = os.path.getsize(json_path) / (1024 * 1024)

        if file_size_mb > 1000:
            safe_print(f"[WARNING] File {json_path.name} c√≥ k√≠ch th∆∞·ªõc {file_size_mb:.2f} MB (> 1GB)")

        # X√≥a file JSONL g·ªëc
        os.remove(jsonl_path)

        safe_print(
            f"[SUCCESS] Ho√†n th√†nh: {file_name} -> {json_path.name} ({line_count} records, {file_size_mb:.2f} MB)")

        return (True, file_name, str(json_path), f"{line_count} records, {file_size_mb:.2f} MB")

    except Exception as e:
        safe_print(f"[ERROR] L·ªói khi x·ª≠ l√Ω {jsonl_path}: {str(e)}")
        return (False, str(jsonl_path), None, str(e))


def merge_json_files(json_files, output_path, batch_index):
    """
    Gh√©p nhi·ªÅu file JSON th√†nh m·ªôt file

    Args:
        json_files: List c√°c ƒë∆∞·ªùng d·∫´n file JSON c·∫ßn gh√©p
        output_path: ƒê∆∞·ªùng d·∫´n file output
        batch_index: Ch·ªâ s·ªë batch ƒë·ªÉ ƒë·∫∑t t√™n

    Returns:
        tuple: (success, output_file, message)
    """
    try:
        safe_print(f"\n[MERGE START] Gh√©p {len(json_files)} files v√†o batch_{batch_index}.json")

        merged_data = []
        total_records = 0

        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    # X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p data l√† list ho·∫∑c dict
                    if isinstance(data, list):
                        merged_data.extend(data)
                        total_records += len(data)
                    else:
                        merged_data.append(data)
                        total_records += 1

                safe_print(
                    f"  [+] ƒê√£ ƒë·ªçc: {Path(json_file).name} ({len(data) if isinstance(data, list) else 1} records)")

            except Exception as e:
                safe_print(f"  [ERROR] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c {json_file}: {str(e)}")
                continue

        if not merged_data:
            return (False, output_path, "No data to merge")

        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # Ghi file merged
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)

        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)

        safe_print(f"[MERGE SUCCESS] ƒê√£ t·∫°o: {Path(output_path).name} ({total_records} records, {file_size_mb:.2f} MB)")

        # X√≥a c√°c file JSON g·ªëc sau khi gh√©p th√†nh c√¥ng
        for json_file in json_files:
            try:
                os.remove(json_file)
                safe_print(f"  [-] ƒê√£ x√≥a: {Path(json_file).name}")
            except Exception as e:
                safe_print(f"  [WARNING] Kh√¥ng x√≥a ƒë∆∞·ª£c {json_file}: {str(e)}")

        return (True, output_path, f"{total_records} records, {file_size_mb:.2f} MB")

    except Exception as e:
        safe_print(f"[MERGE ERROR] L·ªói khi gh√©p files: {str(e)}")
        return (False, output_path, str(e))


def convert_and_merge_jsonl_files():
    """
    T√¨m v√† chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ file JSONL, sau ƒë√≥ gh√©p th√†nh c√°c batch
    """
    # B∆Ø·ªöC 1: Chuy·ªÉn ƒë·ªïi JSONL sang JSON
    print("=" * 80)
    print("B∆Ø·ªöC 1: CHUY·ªÇN ƒê·ªîI JSONL SANG JSON")
    print("=" * 80)

    jsonl_files = glob.glob(os.path.join(INPUT_DIR, "*.jsonl"))

    if not jsonl_files:
        print(f"Kh√¥ng t√¨m th·∫•y file JSONL n√†o trong: {INPUT_DIR}")
        return

    print(f"T√¨m th·∫•y {len(jsonl_files)} file JSONL")
    print(f"S·ªë lu·ªìng x·ª≠ l√Ω: {MAX_WORKERS}")
    print("=" * 80)

    # X·ª≠ l√Ω song song chuy·ªÉn ƒë·ªïi
    conversion_results = {
        'success': [],
        'failed': []
    }

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_file = {
            executor.submit(process_jsonl_file, file_path): file_path
            for file_path in jsonl_files
        }

        for future in as_completed(future_to_file):
            success, file_name, json_path, message = future.result()

            if success and json_path:
                conversion_results['success'].append(json_path)
            else:
                conversion_results['failed'].append((file_name, message))

    # In b√°o c√°o chuy·ªÉn ƒë·ªïi
    print("\n" + "=" * 80)
    print("K·∫æT QU·∫¢ CHUY·ªÇN ƒê·ªîI:")
    print(f"Th√†nh c√¥ng: {len(conversion_results['success'])}/{len(jsonl_files)}")
    print(f"Th·∫•t b·∫°i: {len(conversion_results['failed'])}/{len(jsonl_files)}")

    if conversion_results['failed']:
        print("\nC√°c file th·∫•t b·∫°i:")
        for file_name, error in conversion_results['failed']:
            print(f"  - {file_name}: {error}")

    # B∆Ø·ªöC 2: Gh√©p c√°c file JSON
    if not conversion_results['success']:
        print("\nKh√¥ng c√≥ file JSON n√†o ƒë·ªÉ gh√©p!")
        return

    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 2: GH√âP C√ÅC FILE JSON")
    print("=" * 80)
    print(f"S·ªë file JSON c·∫ßn gh√©p: {len(conversion_results['success'])}")
    print(f"Gh√©p {FILES_PER_MERGE} files th√†nh 1 batch")
    print("=" * 80)

    # Chia th√†nh c√°c batch
    json_files = sorted(conversion_results['success'])
    batches = [json_files[i:i + FILES_PER_MERGE] for i in range(0, len(json_files), FILES_PER_MERGE)]

    print(f"T·ªïng s·ªë batch: {len(batches)}")

    # T·∫°o th∆∞ m·ª•c output
    os.makedirs(OUTPUT_MERGED_DIR, exist_ok=True)

    # Gh√©p t·ª´ng batch
    merge_results = {
        'success': [],
        'failed': []
    }

    for batch_idx, batch_files in enumerate(batches, start=1):
        output_file = os.path.join(OUTPUT_MERGED_DIR, f"merged_batch_{batch_idx:03d}.json")

        success, output_path, message = merge_json_files(batch_files, output_file, batch_idx)

        if success:
            merge_results['success'].append((output_path, message))
        else:
            merge_results['failed'].append((output_path, message))

    # In b√°o c√°o t·ªïng k·∫øt
    print("\n" + "=" * 80)
    print("T·ªîNG K·∫æT CU·ªêI C√ôNG:")
    print("=" * 80)
    print(f"Files JSONL chuy·ªÉn ƒë·ªïi: {len(conversion_results['success'])}/{len(jsonl_files)}")
    print(f"Batches merged: {len(merge_results['success'])}/{len(batches)}")

    if merge_results['failed']:
        print("\nC√°c batch th·∫•t b·∫°i:")
        for file_name, error in merge_results['failed']:
            print(f"  - {file_name}: {error}")

    if merge_results['success']:
        print("\nC√°c file merged ƒë√£ t·∫°o:")
        for file_path, info in merge_results['success']:
            print(f"  ‚úì {Path(file_path).name}: {info}")


if __name__ == "__main__":
    # Ki·ªÉm tra th∆∞ m·ª•c t·ªìn t·∫°i
    if not os.path.exists(INPUT_DIR):
        print(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {INPUT_DIR}")
    else:
        convert_and_merge_jsonl_files()
        print("\nüéâ Ho√†n th√†nh t·∫•t c·∫£!")